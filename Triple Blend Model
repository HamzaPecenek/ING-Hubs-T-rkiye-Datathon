{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ===============================================================\n# üß† ING Hubs T√ºrkiye Datathon ‚Äî Model Training v2\n# ===============================================================\n# Triple blend: CatBoost + LightGBM + XGBoost\n# Meta-learner (Logistic Regression) + Isotonic Calibration\n# Includes official ING metric (Gini/Recall@10/Lift@10 composite)\n# ===============================================================\n\nimport pandas as pd, numpy as np\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.isotonic import IsotonicRegression\nimport warnings, os, gc\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nnp.random.seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:09:48.158789Z","iopub.execute_input":"2025-10-20T12:09:48.159094Z","iopub.status.idle":"2025-10-20T12:09:49.435179Z","shell.execute_reply.started":"2025-10-20T12:09:48.159072Z","shell.execute_reply":"2025-10-20T12:09:49.434347Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ---------------------------------------------------------------\n# üì¶ Load engineered v2 features\n# ---------------------------------------------------------------\ntrain = pd.read_csv(\"/kaggle/input/ing-hubs-datathon-feature-engineered-csvs-v2/train_features_v2.csv\")\ntest  = pd.read_csv(\"/kaggle/input/ing-hubs-datathon-feature-engineered-csvs-v2/test_features_v2.csv\")\n\nprint(\"‚úÖ Data loaded\")\nprint(f\"Train: {train.shape} | Test: {test.shape}\")\n\n# Safety: ensure ref_date is datetime\ntrain[\"ref_date\"] = pd.to_datetime(train[\"ref_date\"])\ntest[\"ref_date\"]  = pd.to_datetime(test[\"ref_date\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:11:09.388965Z","iopub.execute_input":"2025-10-20T12:11:09.389540Z","iopub.status.idle":"2025-10-20T12:11:14.981782Z","shell.execute_reply.started":"2025-10-20T12:11:09.389511Z","shell.execute_reply":"2025-10-20T12:11:14.980117Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Data loaded\nTrain: (133287, 108) | Test: (43006, 107)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ---------------------------------------------------------------\n# üßÆ (Optional) add temporal deltas/ratios 3m vs 6m\n# ---------------------------------------------------------------\ndef add_temporal_deltas(df):\n    # detect base names where both _3m and _6m exist\n    suffixes = [\"_1m\",\"_3m\",\"_6m\"]\n    has_3m = {c[:-3] for c in df.columns if c.endswith(\"_3m\")}\n    bases = [b for b in has_3m if (b + \"_6m\") in df.columns]\n    for base in bases:\n        c3, c6 = base + \"_3m\", base + \"_6m\"\n        df[f\"{base}_delta_3m6m\"]  = df[c3] - df[c6]\n        df[f\"{base}_ratio_3m6m\"]  = df[c3] / df[c6].replace(0, np.nan)\n        df[f\"{base}_growth_3m6m\"] = (df[c3] - df[c6]) / df[c6].replace(0, np.nan)\n    return df.replace([np.inf, -np.inf], np.nan)\n\ntrain = add_temporal_deltas(train)\ntest  = add_temporal_deltas(test)\n\nprint(f\"‚úÖ After deltas: Train {train.shape}, Test {test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:11:29.610849Z","iopub.execute_input":"2025-10-20T12:11:29.611191Z","iopub.status.idle":"2025-10-20T12:11:30.758565Z","shell.execute_reply.started":"2025-10-20T12:11:29.611168Z","shell.execute_reply":"2025-10-20T12:11:30.757314Z"}},"outputs":[{"name":"stdout","text":"‚úÖ After deltas: Train (133287, 192), Test (43006, 191)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ---------------------------------------------------------------\n# ‚è≥ Time-based split (leak-free)\n# ---------------------------------------------------------------\nCUTOFF = \"2018-11-01\"\ntrain_part = train[train[\"ref_date\"] < CUTOFF].copy()\nvalid_part = train[train[\"ref_date\"] >= CUTOFF].copy()\n\ny_train = train_part[\"churn\"].astype(int).values\ny_valid = valid_part[\"churn\"].astype(int).values\n\ndrop_cols = [\"cust_id\",\"ref_date\",\"churn\"]\nX_train = train_part.drop(columns=[c for c in drop_cols if c in train_part.columns])\nX_valid = valid_part.drop(columns=[c for c in drop_cols if c in valid_part.columns])\nX_test  = test.drop(columns=[c for c in [\"cust_id\",\"ref_date\"] if c in test.columns])\n\nprint(f\"üìÖ Train {X_train.shape}, Valid {X_valid.shape}, Test {X_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:11:42.929640Z","iopub.execute_input":"2025-10-20T12:11:42.930023Z","iopub.status.idle":"2025-10-20T12:11:43.230103Z","shell.execute_reply.started":"2025-10-20T12:11:42.929992Z","shell.execute_reply":"2025-10-20T12:11:43.229214Z"}},"outputs":[{"name":"stdout","text":"üìÖ Train (118422, 189), Valid (14865, 189), Test (43006, 189)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ---------------------------------------------------------------\n# üßπ Basic cleaning\n# ---------------------------------------------------------------\n# Align columns (in case deltas created asymmetry)\ncommon_cols = sorted(list(set(X_train.columns) & set(X_valid.columns) & set(X_test.columns)))\nX_train = X_train[common_cols]\nX_valid = X_valid[common_cols]\nX_test  = X_test[common_cols]\n\n# Fill NaNs\nfor df_ in (X_train, X_valid, X_test):\n    # numeric to 0, category to \"Unknown\"\n    num_cols = df_.select_dtypes(include=[np.number]).columns\n    df_[num_cols] = df_[num_cols].fillna(0)\n    # handle any remaining non-numeric as strings/cats\n    non_num = [c for c in df_.columns if c not in num_cols]\n    for c in non_num:\n        df_[c] = df_[c].astype(\"object\").fillna(\"Unknown\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:11:54.711504Z","iopub.execute_input":"2025-10-20T12:11:54.711840Z","iopub.status.idle":"2025-10-20T12:11:55.528432Z","shell.execute_reply.started":"2025-10-20T12:11:54.711807Z","shell.execute_reply":"2025-10-20T12:11:55.527545Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ---------------------------------------------------------------\n# üî¢ Categorical handling\n# ---------------------------------------------------------------\n# Known categoricals from your pipeline; keep only those present\ncat_cols = [c for c in [\"gender\",\"province\",\"religion\",\"work_type\",\"work_sector\",\"season\",\n                        \"province_work_sector\",\"gender_religion\",\"work_type_season\"]\n            if c in common_cols]\n\n# LightGBM wants dtype 'category'; CatBoost uses raw indices; XGBoost needs integer codes.\n# We'll build consistent ordinal codes across train/valid/test for XGBoost & Logistic meta.\ndef make_ordinal_codes(train_df, valid_df, test_df, cat_columns):\n    train_o, valid_o, test_o = train_df.copy(), valid_df.copy(), test_df.copy()\n    code_maps = {}\n    for c in cat_columns:\n        cats = pd.Index(train_o[c].astype(str).unique()).union(\n               pd.Index(valid_o[c].astype(str).unique())).union(\n               pd.Index(test_o[c].astype(str).unique()))\n        cat_type = pd.api.types.CategoricalDtype(categories=cats, ordered=False)\n        train_o[c] = train_o[c].astype(str).astype(cat_type).cat.codes\n        valid_o[c] = valid_o[c].astype(str).astype(cat_type).cat.codes\n        test_o[c]  = test_o[c].astype(str).astype(cat_type).cat.codes\n        code_maps[c] = cats\n    return train_o, valid_o, test_o, code_maps\n\n# Copies for LGB (categorical dtype)\nX_train_lgb = X_train.copy()\nX_valid_lgb = X_valid.copy()\nX_test_lgb  = X_test.copy()\nfor c in cat_cols:\n    for df_ in (X_train_lgb, X_valid_lgb, X_test_lgb):\n        df_[c] = df_[c].astype(\"category\")\n\n# Copies for XGB/meta (ordinal int)\nX_train_xgb, X_valid_xgb, X_test_xgb, _ = make_ordinal_codes(X_train, X_valid, X_test, cat_cols)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:12:11.629801Z","iopub.execute_input":"2025-10-20T12:12:11.630124Z","iopub.status.idle":"2025-10-20T12:12:12.746867Z","shell.execute_reply.started":"2025-10-20T12:12:11.630102Z","shell.execute_reply":"2025-10-20T12:12:12.745762Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ---------------------------------------------------------------\n# ‚öñÔ∏è Class imbalance\n# ---------------------------------------------------------------\nneg, pos = np.bincount(y_train)\nscale_pos_weight = neg / max(pos, 1)\nprint(f\"Class weight ‚âà {scale_pos_weight:.2f}  (pos={pos}, neg={neg})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:12:20.558584Z","iopub.execute_input":"2025-10-20T12:12:20.558900Z","iopub.status.idle":"2025-10-20T12:12:20.565113Z","shell.execute_reply.started":"2025-10-20T12:12:20.558878Z","shell.execute_reply":"2025-10-20T12:12:20.564029Z"}},"outputs":[{"name":"stdout","text":"Class weight ‚âà 6.00  (pos=16906, neg=101516)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ---------------------------------------------------------------\n# üöÄ Model A: CatBoost\n# ---------------------------------------------------------------\nfrom catboost import CatBoostClassifier, Pool\n\ncb = CatBoostClassifier(\n    iterations=3000, learning_rate=0.03, depth=8, l2_leaf_reg=4,\n    random_seed=SEED, loss_function=\"Logloss\", eval_metric=\"AUC\",\n    class_weights=[1.0, float(scale_pos_weight)],\n    od_type=\"Iter\", od_wait=200, task_type=\"CPU\", verbose=False\n)\n\ntrain_pool = Pool(X_train, y_train, cat_features=[X_train.columns.get_loc(c) for c in cat_cols])\nvalid_pool = Pool(X_valid, y_valid, cat_features=[X_valid.columns.get_loc(c) for c in cat_cols])\n\nprint(\"\\nüöÄ Training CatBoost...\")\ncb.fit(train_pool, eval_set=valid_pool, use_best_model=True, verbose=False)\ncb_val  = cb.predict_proba(X_valid)[:,1]\ncb_test = cb.predict_proba(X_test)[:,1]\nprint(\"‚úÖ CatBoost done.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:12:32.142836Z","iopub.execute_input":"2025-10-20T12:12:32.143554Z","iopub.status.idle":"2025-10-20T12:15:54.324770Z","shell.execute_reply.started":"2025-10-20T12:12:32.143525Z","shell.execute_reply":"2025-10-20T12:15:54.323986Z"}},"outputs":[{"name":"stdout","text":"\nüöÄ Training CatBoost...\n‚úÖ CatBoost done.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ---------------------------------------------------------------\n# üöÄ Model B: LightGBM\n# ---------------------------------------------------------------\nimport lightgbm as lgb\n\nlgbm = lgb.LGBMClassifier(\n    n_estimators=5000, learning_rate=0.02, num_leaves=128,\n    subsample=0.8, colsample_bytree=0.8, min_data_in_leaf=30,\n    reg_alpha=0.1, reg_lambda=0.5, objective=\"binary\",\n    metric=\"auc\", random_state=SEED, n_jobs=-1,\n    scale_pos_weight=scale_pos_weight\n)\n\nprint(\"\\nüöÄ Training LightGBM...\")\nlgbm.fit(X_train_lgb, y_train,\n         eval_set=[(X_valid_lgb, y_valid)],\n         eval_metric=\"auc\",\n         callbacks=[lgb.early_stopping(300, verbose=False)])\nlgb_val  = lgbm.predict_proba(X_valid_lgb)[:,1]\nlgb_test = lgbm.predict_proba(X_test_lgb)[:,1]\nprint(\"‚úÖ LightGBM done.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:18:03.658495Z","iopub.execute_input":"2025-10-20T12:18:03.658820Z","iopub.status.idle":"2025-10-20T12:18:45.488916Z","shell.execute_reply.started":"2025-10-20T12:18:03.658797Z","shell.execute_reply":"2025-10-20T12:18:45.488037Z"}},"outputs":[{"name":"stdout","text":"\nüöÄ Training LightGBM...\n[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n[LightGBM] [Info] Number of positive: 16906, number of negative: 101516\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157231 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 32161\n[LightGBM] [Info] Number of data points in the train set: 118422, number of used features: 179\n[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.142761 -> initscore=-1.792548\n[LightGBM] [Info] Start training from score -1.792548\n[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n‚úÖ LightGBM done.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ---------------------------------------------------------------\n# üöÄ Model C: XGBoost\n# ---------------------------------------------------------------\nimport xgboost as xgb\n\nxgbm = xgb.XGBClassifier(\n    n_estimators=3000, learning_rate=0.03, max_depth=8,\n    subsample=0.8, colsample_bytree=0.8,\n    reg_alpha=0.1, reg_lambda=1.0,\n    eval_metric=\"logloss\", tree_method=\"hist\",\n    scale_pos_weight=scale_pos_weight,\n    random_state=SEED, n_jobs=-1\n)\n\nprint(\"\\nüöÄ Training XGBoost...\")\nxgbm.fit(X_train_xgb, y_train,\n         eval_set=[(X_valid_xgb, y_valid)],\n         early_stopping_rounds=200, verbose=False)\nxgb_val  = xgbm.predict_proba(X_valid_xgb)[:,1]\nxgb_test = xgbm.predict_proba(X_test_xgb)[:,1]\nprint(\"‚úÖ XGBoost done.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:23:51.716058Z","iopub.execute_input":"2025-10-20T12:23:51.717881Z","iopub.status.idle":"2025-10-20T12:29:03.423599Z","shell.execute_reply.started":"2025-10-20T12:23:51.717839Z","shell.execute_reply":"2025-10-20T12:29:03.422813Z"}},"outputs":[{"name":"stdout","text":"\nüöÄ Training XGBoost...\n‚úÖ XGBoost done.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ---------------------------------------------------------------\n# üßÆ Official ING Composite Metric\n# ---------------------------------------------------------------\ndef recall_at_k(y_true, y_prob, k=0.1):\n    y_true = np.asarray(y_true)\n    y_prob = np.asarray(y_prob)\n    n = len(y_true)\n    m = max(1, int(np.round(k * n)))\n    order = np.argsort(-y_prob, kind=\"mergesort\")\n    top = order[:m]\n    tp_at_k = y_true[top].sum()\n    P = y_true.sum()\n    return float(tp_at_k / P) if P > 0 else 0.0\n\ndef lift_at_k(y_true, y_prob, k=0.1):\n    y_true = np.asarray(y_true)\n    y_prob = np.asarray(y_prob)\n    n = len(y_true)\n    m = max(1, int(np.round(k * n)))\n    order = np.argsort(-y_prob, kind=\"mergesort\")\n    top = order[:m]\n    tp_at_k = y_true[top].sum()\n    precision_at_k = tp_at_k / m\n    prevalence = y_true.mean()\n    return float(precision_at_k / prevalence) if prevalence > 0 else 0.0\n\ndef convert_auc_to_gini(auc): return 2 * auc - 1\n\ndef ing_hubs_datathon_metric(y_true, y_prob):\n    score_weights = {\"gini\": 0.4, \"recall_at_10perc\": 0.3, \"lift_at_10perc\": 0.3}\n    baseline = {\"roc_auc\": 0.6925726757936908,\n                \"recall_at_10perc\": 0.18469015795868773,\n                \"lift_at_10perc\": 1.847159286784029}\n    roc_auc = roc_auc_score(y_true, y_prob)\n    recall10 = recall_at_k(y_true, y_prob, 0.1)\n    lift10 = lift_at_k(y_true, y_prob, 0.1)\n    gini_new, gini_base = convert_auc_to_gini(roc_auc), convert_auc_to_gini(baseline[\"roc_auc\"])\n    score = (gini_new/gini_base)*score_weights[\"gini\"] + \\\n            (recall10/baseline[\"recall_at_10perc\"])*score_weights[\"recall_at_10perc\"] + \\\n            (lift10/baseline[\"lift_at_10perc\"])*score_weights[\"lift_at_10perc\"]\n    return score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:29:10.420368Z","iopub.execute_input":"2025-10-20T12:29:10.420685Z","iopub.status.idle":"2025-10-20T12:29:10.432754Z","shell.execute_reply.started":"2025-10-20T12:29:10.420663Z","shell.execute_reply":"2025-10-20T12:29:10.431250Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# ---------------------------------------------------------------\n# üîó Meta-learner (Stacked blend on validation predictions)\n# ---------------------------------------------------------------\n# Train logistic regression on validation predictions of the 3 models\nZ_valid = np.vstack([cb_val, lgb_val, xgb_val]).T\nZ_test  = np.vstack([cb_test, lgb_test, xgb_test]).T\n\nmeta = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=SEED)\nmeta.fit(Z_valid, y_valid)\n\nmeta_val  = meta.predict_proba(Z_valid)[:,1]\nmeta_test = meta.predict_proba(Z_test)[:,1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:29:15.193518Z","iopub.execute_input":"2025-10-20T12:29:15.193849Z","iopub.status.idle":"2025-10-20T12:29:15.268708Z","shell.execute_reply.started":"2025-10-20T12:29:15.193827Z","shell.execute_reply":"2025-10-20T12:29:15.267927Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# ---------------------------------------------------------------\n# üéØ Calibration (Isotonic on meta output)\n# ---------------------------------------------------------------\niso = IsotonicRegression(out_of_bounds='clip')\niso.fit(meta_val, y_valid)\nmeta_val_cal  = iso.predict(meta_val)\nmeta_test_cal = iso.predict(meta_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:29:17.241192Z","iopub.execute_input":"2025-10-20T12:29:17.241553Z","iopub.status.idle":"2025-10-20T12:29:17.259327Z","shell.execute_reply.started":"2025-10-20T12:29:17.241529Z","shell.execute_reply":"2025-10-20T12:29:17.258131Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# ---------------------------------------------------------------\n# üìä Validation metrics\n# ---------------------------------------------------------------\ndef summarize(name, y_true, y_prob):\n    auc = roc_auc_score(y_true, y_prob)\n    ll  = log_loss(y_true, y_prob)\n    rec = recall_at_k(y_true, y_prob, 0.1)\n    lif = lift_at_k(y_true, y_prob, 0.1)\n    ing = ing_hubs_datathon_metric(y_true, y_prob)\n    return dict(model=name, AUC=auc, LogLoss=ll, Recall10=rec, Lift10=lif, ING=ing)\n\nsummary_rows = []\nsummary_rows.append(summarize(\"CatBoost\", y_valid, cb_val))\nsummary_rows.append(summarize(\"LightGBM\", y_valid, lgb_val))\nsummary_rows.append(summarize(\"XGBoost\", y_valid, xgb_val))\nsummary_rows.append(summarize(\"Meta (uncal.)\", y_valid, meta_val))\nsummary_rows.append(summarize(\"Meta (cal.)\", y_valid, meta_val_cal))\n\nsummary = pd.DataFrame(summary_rows)\nprint(\"\\nüìä Validation Summary\")\ndisplay(summary.sort_values(\"ING\", ascending=False))\n\nbest_ing = summary.sort_values(\"ING\", ascending=False).iloc[0]\nprint(f\"\\nüèÅ Best local (by ING metric): {best_ing['model']}  ‚Üí ING={best_ing['ING']:.5f} | \"\n      f\"AUC={best_ing['AUC']:.5f} | LogLoss={best_ing['LogLoss']:.5f} | \"\n      f\"Recall@10={best_ing['Recall10']:.4f} | Lift@10={best_ing['Lift10']:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:29:19.568168Z","iopub.execute_input":"2025-10-20T12:29:19.568569Z","iopub.status.idle":"2025-10-20T12:29:19.718045Z","shell.execute_reply.started":"2025-10-20T12:29:19.568546Z","shell.execute_reply":"2025-10-20T12:29:19.717190Z"}},"outputs":[{"name":"stdout","text":"\nüìä Validation Summary\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"           model       AUC   LogLoss  Recall10    Lift10       ING\n4    Meta (cal.)  0.716950  0.353508  0.219959  2.200333  1.165283\n0       CatBoost  0.714048  0.577045  0.214358  2.144306  1.141059\n3  Meta (uncal.)  0.714336  0.619746  0.213849  2.139212  1.140003\n1       LightGBM  0.706504  0.464783  0.212322  2.123932  1.118772\n2        XGBoost  0.682709  0.387601  0.190937  1.910011  0.999867","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>AUC</th>\n      <th>LogLoss</th>\n      <th>Recall10</th>\n      <th>Lift10</th>\n      <th>ING</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>Meta (cal.)</td>\n      <td>0.716950</td>\n      <td>0.353508</td>\n      <td>0.219959</td>\n      <td>2.200333</td>\n      <td>1.165283</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>CatBoost</td>\n      <td>0.714048</td>\n      <td>0.577045</td>\n      <td>0.214358</td>\n      <td>2.144306</td>\n      <td>1.141059</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Meta (uncal.)</td>\n      <td>0.714336</td>\n      <td>0.619746</td>\n      <td>0.213849</td>\n      <td>2.139212</td>\n      <td>1.140003</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LightGBM</td>\n      <td>0.706504</td>\n      <td>0.464783</td>\n      <td>0.212322</td>\n      <td>2.123932</td>\n      <td>1.118772</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>XGBoost</td>\n      <td>0.682709</td>\n      <td>0.387601</td>\n      <td>0.190937</td>\n      <td>1.910011</td>\n      <td>0.999867</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nüèÅ Best local (by ING metric): Meta (cal.)  ‚Üí ING=1.16528 | AUC=0.71695 | LogLoss=0.35351 | Recall@10=0.2200 | Lift@10=2.200\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ---------------------------------------------------------------\n# üíæ Test predictions & submissions\n# ---------------------------------------------------------------\nsub_base = pd.DataFrame({\"cust_id\": test[\"cust_id\"]})\n\n# Save both, but recommend calibrated\nsub_base[\"churn\"] = meta_test\nsub_base.to_csv(\"submission_triple_blend_uncalibrated.csv\", index=False)\n\nsub_base[\"churn\"] = meta_test_cal\nsub_base.to_csv(\"submission_triple_blend_calibrated.csv\", index=False)\n\nprint(\"\\nüíæ Saved:\")\nprint(\" - submission_triple_blend_uncalibrated.csv\")\nprint(\" - submission_triple_blend_calibrated.csv  ‚úÖ recommended\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:29:29.026268Z","iopub.execute_input":"2025-10-20T12:29:29.027085Z","iopub.status.idle":"2025-10-20T12:29:29.236557Z","shell.execute_reply.started":"2025-10-20T12:29:29.027060Z","shell.execute_reply":"2025-10-20T12:29:29.235518Z"}},"outputs":[{"name":"stdout","text":"\nüíæ Saved:\n - submission_triple_blend_uncalibrated.csv\n - submission_triple_blend_calibrated.csv  ‚úÖ recommended\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}