{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ===============================================================\n# ğŸ§± ING Hubs TÃ¼rkiye Datathon â€” Feature Engineering v2\n# ===============================================================\n# Adds volatility, acceleration, and categorical interaction features\n# Author: Ali Hamza PeÃ§enek\n# ===============================================================\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T11:52:48.393932Z","iopub.execute_input":"2025-10-20T11:52:48.394210Z","iopub.status.idle":"2025-10-20T11:52:48.713842Z","shell.execute_reply.started":"2025-10-20T11:52:48.394187Z","shell.execute_reply":"2025-10-20T11:52:48.713059Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ===============================================================\n# ğŸ“¦ Load Raw Data\n# ===============================================================\ncustomers = pd.read_csv(\"/kaggle/input/ing-hubs-turkiye-datathon/customers.csv\")\ncustomer_history = pd.read_csv(\"/kaggle/input/ing-hubs-turkiye-datathon/customer_history.csv\")\nreference_data = pd.read_csv(\"/kaggle/input/ing-hubs-turkiye-datathon/referance_data.csv\")\nreference_data_test = pd.read_csv(\"/kaggle/input/ing-hubs-turkiye-datathon/referance_data_test.csv\")\n\nprint(\"âœ… Data loaded successfully!\")\nfor name, df in {\n    \"customers\": customers,\n    \"customer_history\": customer_history,\n    \"reference_data\": reference_data,\n    \"reference_data_test\": reference_data_test\n}.items():\n    print(f\"{name:25} â†’ {df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T11:53:26.637763Z","iopub.execute_input":"2025-10-20T11:53:26.638189Z","iopub.status.idle":"2025-10-20T11:53:32.637371Z","shell.execute_reply.started":"2025-10-20T11:53:26.638166Z","shell.execute_reply":"2025-10-20T11:53:32.636487Z"}},"outputs":[{"name":"stdout","text":"âœ… Data loaded successfully!\ncustomers                 â†’ (176293, 8)\ncustomer_history          â†’ (5359609, 7)\nreference_data            â†’ (133287, 3)\nreference_data_test       â†’ (43006, 2)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ===============================================================\n# âš™ï¸ Helper Function â€” Build Features\n# ===============================================================\ndef build_features(reference_df, customer_history, customers, windows=[1, 3, 6]):\n    df_ref = reference_df.copy()\n    df_hist = customer_history.copy()\n    df_ref[\"ref_date\"] = pd.to_datetime(df_ref[\"ref_date\"])\n    df_hist[\"date\"] = pd.to_datetime(df_hist[\"date\"])\n\n    num_cols = [\n        \"mobile_eft_all_cnt\", \"mobile_eft_all_amt\",\n        \"cc_transaction_all_cnt\", \"cc_transaction_all_amt\",\n        \"active_product_category_nbr\"\n    ]\n    feature_list = []\n\n    print(\"ğŸ§® Multi-window aggregates...\")\n    for w in tqdm(windows, desc=\"Windows\"):\n        temp = df_ref[[\"cust_id\", \"ref_date\"]].copy()\n        merged = temp.merge(df_hist, on=\"cust_id\", how=\"left\")\n        mask = (merged[\"date\"] < merged[\"ref_date\"]) & (\n            merged[\"date\"] >= merged[\"ref_date\"] - pd.DateOffset(months=w)\n        )\n        merged = merged[mask]\n\n        agg = merged.groupby(\"cust_id\")[num_cols].agg([\"mean\", \"std\", \"min\", \"max\"])\n        agg.columns = [f\"{col}_{stat}_{w}m\" for col, stat in agg.columns]\n        agg = agg.reset_index()\n\n        # Volatility = std/mean\n        for col in num_cols:\n            agg[f\"{col}_volatility_{w}m\"] = agg[f\"{col}_std_{w}m\"] / (agg[f\"{col}_mean_{w}m\"] + 1e-6)\n\n        temp = temp.merge(agg, on=\"cust_id\", how=\"left\")\n        feature_list.append(temp)\n\n    print(\"ğŸ”— Merging window features...\")\n    features = feature_list[0]\n    for extra in feature_list[1:]:\n        features = features.merge(extra, on=[\"cust_id\", \"ref_date\"], how=\"left\")\n\n    # Ratios\n    print(\"âš™ï¸ Derived ratios...\")\n    for w in windows:\n        features[f\"eft_to_cc_cnt_ratio_{w}m\"] = (\n            features[f\"mobile_eft_all_cnt_mean_{w}m\"] /\n            (features[f\"cc_transaction_all_cnt_mean_{w}m\"] + 1)\n        )\n        features[f\"eft_to_cc_amt_ratio_{w}m\"] = (\n            features[f\"mobile_eft_all_amt_mean_{w}m\"] /\n            (features[f\"cc_transaction_all_amt_mean_{w}m\"] + 1)\n        )\n        features[f\"amt_per_tx_{w}m\"] = (\n            (features[f\"mobile_eft_all_amt_mean_{w}m\"] + features[f\"cc_transaction_all_amt_mean_{w}m\"]) /\n            (features[f\"mobile_eft_all_cnt_mean_{w}m\"] + features[f\"cc_transaction_all_cnt_mean_{w}m\"] + 1)\n        )\n\n    # Slopes + Acceleration\n    print(\"ğŸ“ˆ Computing slopes and acceleration...\")\n    df_hist[\"month\"] = df_hist[\"date\"].dt.to_period(\"M\")\n    monthly = df_hist.groupby([\"cust_id\", \"month\"]).agg({\n        \"mobile_eft_all_cnt\": \"mean\",\n        \"cc_transaction_all_cnt\": \"mean\",\n        \"cc_transaction_all_amt\": \"mean\"\n    }).reset_index()\n\n    monthly[\"month_idx\"] = monthly.groupby(\"cust_id\")[\"month\"].transform(\n        lambda x: (x - x.min()).apply(lambda p: p.n)\n    ).astype(int)\n\n    def compute_slope(x, y):\n        x_mean, y_mean = np.mean(x), np.mean(y)\n        num = np.sum((x - x_mean) * (y - y_mean))\n        den = np.sum((x - x_mean) ** 2)\n        return num / den if den != 0 else 0.0\n\n    slope_data = []\n    for col in [\"mobile_eft_all_cnt\", \"cc_transaction_all_cnt\", \"cc_transaction_all_amt\"]:\n        slope = monthly.groupby(\"cust_id\").apply(\n            lambda g: compute_slope(g[\"month_idx\"].values, g[col].fillna(0).values)\n        ).rename(f\"{col}_slope_6m\")\n        slope_data.append(slope)\n\n    slope_df = pd.concat(slope_data, axis=1).reset_index()\n\n    # Acceleration = slope difference between last 2 periods\n    print(\"âš¡ Adding acceleration features...\")\n    accel_df = monthly.groupby(\"cust_id\").apply(\n        lambda g: pd.Series({\n            f\"{col}_acceleration\": (\n                compute_slope(g[\"month_idx\"].values[-3:], g[col].values[-3:]) -\n                compute_slope(g[\"month_idx\"].values[:-3], g[col].values[:-3])\n            )\n            for col in [\"mobile_eft_all_cnt\", \"cc_transaction_all_cnt\", \"cc_transaction_all_amt\"]\n        })\n    ).reset_index()\n\n    features = features.merge(slope_df, on=\"cust_id\", how=\"left\")\n    features = features.merge(accel_df, on=\"cust_id\", how=\"left\")\n\n    # Demographics\n    print(\"ğŸ‘¤ Merging customer demographics...\")\n    demo = customers.copy()\n    demo[\"age_tenure_interaction\"] = demo[\"age\"] * demo[\"tenure\"]\n    features = features.merge(demo, on=\"cust_id\", how=\"left\")\n\n    # Temporal context\n    features[\"month_of_ref_date\"] = features[\"ref_date\"].dt.month\n    features[\"quarter_of_ref_date\"] = features[\"ref_date\"].dt.quarter\n    features[\"ref_year\"] = features[\"ref_date\"].dt.year\n\n    season_map = {\n        12: \"Winter\", 1: \"Winter\", 2: \"Winter\",\n        3: \"Spring\", 4: \"Spring\", 5: \"Spring\",\n        6: \"Summer\", 7: \"Summer\", 8: \"Summer\",\n        9: \"Fall\", 10: \"Fall\", 11: \"Fall\"\n    }\n    features[\"season\"] = features[\"month_of_ref_date\"].map(season_map)\n\n    # Clean-up\n    features = features.replace([np.inf, -np.inf], np.nan)\n    features = features.fillna(0)\n\n    # Preserve churn if present\n    if \"churn\" in df_ref.columns:\n        features = features.merge(df_ref[[\"cust_id\", \"ref_date\", \"churn\"]],\n                                  on=[\"cust_id\", \"ref_date\"], how=\"left\")\n\n    print(\"âœ… Base features done.\")\n    return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T11:53:52.984816Z","iopub.execute_input":"2025-10-20T11:53:52.985555Z","iopub.status.idle":"2025-10-20T11:53:53.004668Z","shell.execute_reply.started":"2025-10-20T11:53:52.985526Z","shell.execute_reply":"2025-10-20T11:53:53.003616Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ===============================================================\n# ğŸš€ Generate v2 Features\n# ===============================================================\nprint(\"\\nğŸš€ Building v2 train/test features...\")\ntrain_v2 = build_features(reference_data, customer_history, customers)\ntest_v2  = build_features(reference_data_test, customer_history, customers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T11:54:11.651811Z","iopub.execute_input":"2025-10-20T11:54:11.652116Z","iopub.status.idle":"2025-10-20T12:04:00.419607Z","shell.execute_reply.started":"2025-10-20T11:54:11.652094Z","shell.execute_reply":"2025-10-20T12:04:00.418802Z"}},"outputs":[{"name":"stdout","text":"\nğŸš€ Building v2 train/test features...\nğŸ§® Multi-window aggregates...\n","output_type":"stream"},{"name":"stderr","text":"Windows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"ğŸ”— Merging window features...\nâš™ï¸ Derived ratios...\nğŸ“ˆ Computing slopes and acceleration...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3192984746.py:79: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  slope = monthly.groupby(\"cust_id\").apply(\n/tmp/ipykernel_37/3192984746.py:79: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  slope = monthly.groupby(\"cust_id\").apply(\n/tmp/ipykernel_37/3192984746.py:79: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  slope = monthly.groupby(\"cust_id\").apply(\n","output_type":"stream"},{"name":"stdout","text":"âš¡ Adding acceleration features...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3192984746.py:88: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  accel_df = monthly.groupby(\"cust_id\").apply(\n","output_type":"stream"},{"name":"stdout","text":"ğŸ‘¤ Merging customer demographics...\nâœ… Base features done.\nğŸ§® Multi-window aggregates...\n","output_type":"stream"},{"name":"stderr","text":"Windows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"ğŸ”— Merging window features...\nâš™ï¸ Derived ratios...\nğŸ“ˆ Computing slopes and acceleration...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3192984746.py:79: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  slope = monthly.groupby(\"cust_id\").apply(\n/tmp/ipykernel_37/3192984746.py:79: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  slope = monthly.groupby(\"cust_id\").apply(\n/tmp/ipykernel_37/3192984746.py:79: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  slope = monthly.groupby(\"cust_id\").apply(\n","output_type":"stream"},{"name":"stdout","text":"âš¡ Adding acceleration features...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3192984746.py:88: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  accel_df = monthly.groupby(\"cust_id\").apply(\n","output_type":"stream"},{"name":"stdout","text":"ğŸ‘¤ Merging customer demographics...\nâœ… Base features done.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ===============================================================\n# ğŸ§© Add Categorical Interaction Features\n# ===============================================================\nprint(\"\\nğŸ§© Adding categorical interaction features...\")\ncat_pairs = [\n    (\"province\", \"work_sector\"),\n    (\"gender\", \"religion\"),\n    (\"work_type\", \"season\")\n]\n\nfor c1, c2 in cat_pairs:\n    if c1 in train_v2.columns and c2 in train_v2.columns:\n        train_v2[f\"{c1}_{c2}\"] = train_v2[c1].astype(str) + \"_\" + train_v2[c2].astype(str)\n        test_v2[f\"{c1}_{c2}\"]  = test_v2[c1].astype(str) + \"_\" + test_v2[c2].astype(str)\n\nprint(f\"âœ… Added {len(cat_pairs)} interaction pairs.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:04:54.314274Z","iopub.execute_input":"2025-10-20T12:04:54.314595Z","iopub.status.idle":"2025-10-20T12:04:54.415959Z","shell.execute_reply.started":"2025-10-20T12:04:54.314570Z","shell.execute_reply":"2025-10-20T12:04:54.415058Z"}},"outputs":[{"name":"stdout","text":"\nğŸ§© Adding categorical interaction features...\nâœ… Added 3 interaction pairs.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ===============================================================\n# ğŸ’¾ Save\n# ===============================================================\ntrain_v2.to_csv(\"train_features_v2.csv\", index=False)\ntest_v2.to_csv(\"test_features_v2.csv\", index=False)\n\nprint(\"\\nğŸ’¾ Saved: train_features_v2.csv & test_features_v2.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:05:02.058734Z","iopub.execute_input":"2025-10-20T12:05:02.059091Z","iopub.status.idle":"2025-10-20T12:05:20.963661Z","shell.execute_reply.started":"2025-10-20T12:05:02.059063Z","shell.execute_reply":"2025-10-20T12:05:20.962858Z"}},"outputs":[{"name":"stdout","text":"\nğŸ’¾ Saved: train_features_v2.csv & test_features_v2.csv\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}