{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116092,"databundleVersionId":13860829,"sourceType":"competition"},{"sourceId":13424641,"sourceType":"datasetVersion","datasetId":8520630}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ===============================================================\n# üß† CATBOOST MODEL ‚Äî ING Hubs T√ºrkiye Datathon (End-to-End)\n# ===============================================================\n# What this notebook does:\n# 1) Load engineered features\n# 2) Leak-free time-based split & folds\n# 3) CatBoost TimeCV grid search (depth, lr, l2)\n# 4) Visualize AUC/LogLoss heatmaps + 3D view\n# 5) Train a baseline model on a single split (for a quick local check)\n# 6) Auto-retrain final model on ALL data with best tuned params\n# 7) Save submission files\n\nimport os, warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom itertools import product\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom catboost import CatBoostClassifier, Pool\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nos.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n\n# ---------------------------------------------------------------\n# üß© Config\n# ---------------------------------------------------------------\nRANDOM_SEED = 42\nDATE_COL = \"ref_date\"\nID_COL   = \"cust_id\"\nTARGET   = \"churn\"\n\n# Paths (adjust if needed)\nTRAIN_PATH = \"/kaggle/input/feature-engineered-csvs/train_features(1).csv\"\nTEST_PATH  = \"/kaggle/input/feature-engineered-csvs/test_features(1).csv\"\n\n# Categorical columns present in engineering\nBASE_CAT_COLS = [\"gender\", \"province\", \"religion\", \"work_type\", \"work_sector\", \"season\"]\n\n# ---------------------------------------------------------------\n# 1) üì¶ Load Engineered Features\n# ---------------------------------------------------------------\ntrain = pd.read_csv(TRAIN_PATH)\ntest  = pd.read_csv(TEST_PATH)\n\nprint(\"‚úÖ Data loaded successfully!\")\nprint(f\"Train shape: {train.shape}\")\nprint(f\"Test shape:  {test.shape}\")\n\n# ---------------------------------------------------------------\n# 2) ‚è≥ Single holdout ‚Äî Time-based Split (Leak-Free)\n# ---------------------------------------------------------------\ntrain[DATE_COL] = pd.to_datetime(train[DATE_COL])\ntest[DATE_COL]  = pd.to_datetime(test[DATE_COL])\n\n# Cutoff matches your earlier LightGBM setup\nCUTOFF = \"2018-11-01\"\ntrain_part = train[train[DATE_COL] < CUTOFF]\nvalid_part = train[train[DATE_COL] >= CUTOFF]\n\nX_train = train_part.drop(columns=[ID_COL, DATE_COL, TARGET])\ny_train = train_part[TARGET]\nX_valid = valid_part.drop(columns=[ID_COL, DATE_COL, TARGET])\ny_valid = valid_part[TARGET]\n\n# Categorical columns that actually exist\ncat_cols = [c for c in BASE_CAT_COLS if c in X_train.columns]\n# CatBoost is happiest with indices for cat_features\ncat_idx = [X_train.columns.get_loc(c) for c in cat_cols]\n\nprint(f\"\\nüìÖ Train size: {X_train.shape}, Valid size: {X_valid.shape}\")\nprint(f\"üß© Using {len(cat_cols)} categorical features: {cat_cols}\")\n\n# ---------------------------------------------------------------\n# Small helper: class weight (imbalance) calculator\n# ---------------------------------------------------------------\ndef compute_scale_pos_weight(y):\n    neg, pos = np.bincount(y.astype(int))\n    return max(1.0, neg / max(1, pos))\n\n# ---------------------------------------------------------------\n# 3) üïì Build Time-based Cross-Validation Folds (Leak-Free)\n# ---------------------------------------------------------------\ndef build_time_folds(df, date_col=DATE_COL, n_splits=4, val_months=1, gap_months=1, min_train_blocks=3):\n    months = pd.to_datetime(df[date_col]).dt.to_period(\"M\")\n    uniq = months.sort_values().unique()\n    folds = []\n    for val_end_pos in range(min_train_blocks + gap_months + val_months, len(uniq) + 1):\n        val_start_pos = val_end_pos - val_months\n        gap_start_pos = val_start_pos - gap_months\n        train_end_pos = gap_start_pos\n        if train_end_pos < min_train_blocks:\n            continue\n        folds.append((uniq[:train_end_pos], uniq[val_start_pos:val_end_pos]))\n    folds = folds[-n_splits:]\n    idx_pairs = []\n    for train_months, valid_months in folds:\n        tr_idx = months[months.isin(train_months)].index.values\n        va_idx = months[months.isin(valid_months)].index.values\n        idx_pairs.append((tr_idx, va_idx))\n    print(f\"‚úÖ Built {len(idx_pairs)} time folds\")\n    return idx_pairs\n\nfolds = build_time_folds(train, date_col=DATE_COL, n_splits=4, val_months=1, gap_months=1, min_train_blocks=3)\n\n# ---------------------------------------------------------------\n# 4) üß≠ CatBoost Grid Search with TimeCV\n# ---------------------------------------------------------------\nparam_grid = {\n    \"depth\": [6, 8, 10],\n    \"learning_rate\": [0.03, 0.05, 0.07],\n    \"l2_leaf_reg\": [3, 5, 7]\n}\nparam_combos = list(product(param_grid[\"depth\"], param_grid[\"learning_rate\"], param_grid[\"l2_leaf_reg\"]))\nprint(f\"\\nüîç Evaluating {len(param_combos)} parameter combinations over {len(folds)} folds...\\n\")\n\nresults = []\nbest_auc, best_params = -1, None\n\nfor depth, lr, l2 in param_combos:\n    fold_aucs, fold_loglosses = [], []\n\n    for fold_i, (tr_idx, va_idx) in enumerate(folds, 1):\n        X_tr = train.iloc[tr_idx].drop(columns=[ID_COL, DATE_COL, TARGET])\n        y_tr = train.iloc[tr_idx][TARGET]\n        X_va = train.iloc[va_idx].drop(columns=[ID_COL, DATE_COL, TARGET])\n        y_va = train.iloc[va_idx][TARGET]\n\n        spw = compute_scale_pos_weight(y_tr)\n        train_pool = Pool(X_tr, y_tr, cat_features=cat_idx)\n        valid_pool = Pool(X_va, y_va, cat_features=cat_idx)\n\n        model = CatBoostClassifier(\n            iterations=2000,\n            depth=depth,\n            learning_rate=lr,\n            l2_leaf_reg=l2,\n            random_seed=RANDOM_SEED,\n            loss_function=\"Logloss\",\n            eval_metric=\"AUC\",\n            class_weights=[1.0, spw],\n            od_type=\"Iter\",\n            od_wait=200,     # early stopping\n            task_type=\"CPU\",\n            verbose=False\n        )\n\n        model.fit(train_pool, eval_set=valid_pool, use_best_model=True, verbose=False)\n        preds = model.predict_proba(X_va)[:, 1]\n\n        fold_auc = roc_auc_score(y_va, preds)\n        fold_logloss = log_loss(y_va, preds)\n        fold_aucs.append(fold_auc)\n        fold_loglosses.append(fold_logloss)\n\n        print(f\"  Fold {fold_i}: AUC={fold_auc:.5f}  LogLoss={fold_logloss:.5f}\")\n\n    mean_auc, mean_logloss = float(np.mean(fold_aucs)), float(np.mean(fold_loglosses))\n    results.append((depth, lr, l2, mean_auc, mean_logloss))\n    print(f\"‚û°Ô∏è depth={depth:<2} lr={lr:<4} l2={l2:<3} ‚Üí mean AUC={mean_auc:.5f}, mean LogLoss={mean_logloss:.5f}\\n\")\n\n    if mean_auc > best_auc:\n        best_auc, best_params = mean_auc, (depth, lr, l2)\n\n# Save grid search results\ngrid_df = pd.DataFrame(results, columns=[\"depth\", \"learning_rate\", \"l2_leaf_reg\", \"AUC\", \"LogLoss\"])\ngrid_df.to_csv(\"catboost_timecv_results.csv\", index=False)\n\nprint(\"\\nüèÅ Best parameters (based on mean AUC):\")\nprint(f\"depth={best_params[0]}, learning_rate={best_params[1]}, l2_leaf_reg={best_params[2]}\")\nprint(f\"‚úÖ Best Mean AUC: {best_auc:.5f}\")\nprint(\"üíæ Saved catboost_timecv_results.csv (full tuning log)\")\n\n# ---------------------------------------------------------------\n# 5) üìä Visualize Grid Search (AUC & LogLoss Heatmaps + 3D)\n# ---------------------------------------------------------------\ntry:\n    import seaborn as sns\n    print(\"\\nüé® Plotting heatmaps...\")\n\n    plt.figure(figsize=(14, 5))\n    for i, l2 in enumerate(sorted(grid_df[\"l2_leaf_reg\"].unique()), 1):\n        pivot_auc = grid_df[grid_df[\"l2_leaf_reg\"] == l2].pivot(index=\"depth\", columns=\"learning_rate\", values=\"AUC\")\n        plt.subplot(1, len(sorted(grid_df[\"l2_leaf_reg\"].unique())), i)\n        sns.heatmap(pivot_auc, annot=True, fmt=\".4f\", cmap=\"YlGnBu\")\n        plt.title(f\"AUC Heatmap (l2_leaf_reg={l2})\")\n        plt.xlabel(\"learning_rate\")\n        plt.ylabel(\"depth\")\n    plt.tight_layout()\n    plt.show()\n\n    plt.figure(figsize=(14, 5))\n    for i, l2 in enumerate(sorted(grid_df[\"l2_leaf_reg\"].unique()), 1):\n        pivot_log = grid_df[grid_df[\"l2_leaf_reg\"] == l2].pivot(index=\"depth\", columns=\"learning_rate\", values=\"LogLoss\")\n        plt.subplot(1, len(sorted(grid_df[\"l2_leaf_reg\"].unique())), i)\n        sns.heatmap(pivot_log, annot=True, fmt=\".4f\", cmap=\"Reds_r\")\n        plt.title(f\"LogLoss Heatmap (l2_leaf_reg={l2})\")\n        plt.xlabel(\"learning_rate\")\n        plt.ylabel(\"depth\")\n    plt.tight_layout()\n    plt.show()\n\n    from mpl_toolkits.mplot3d import Axes3D  # noqa\n    fig = plt.figure(figsize=(8,6))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(\n        grid_df[\"depth\"],\n        grid_df[\"learning_rate\"],\n        grid_df[\"AUC\"],\n        c=grid_df[\"AUC\"], cmap=\"viridis\", s=70\n    )\n    ax.set_xlabel(\"Depth\")\n    ax.set_ylabel(\"Learning Rate\")\n    ax.set_zlabel(\"AUC\")\n    ax.set_title(\"3D Visualization of AUC across Parameters\")\n    plt.tight_layout()\n    plt.show()\nexcept Exception as e:\n    print(\"Visualization step skipped:\", e)\n\n# ---------------------------------------------------------------\n# 6) üß™ Baseline Single-Split Model (quick local sanity check)\n# ---------------------------------------------------------------\nspw_single = compute_scale_pos_weight(y_train)\ntrain_pool = Pool(X_train, y_train, cat_features=cat_idx)\nvalid_pool = Pool(X_valid, y_valid, cat_features=cat_idx)\n\nbaseline = CatBoostClassifier(\n    iterations=3000,\n    learning_rate=0.05,\n    depth=8,\n    l2_leaf_reg=5,\n    random_seed=RANDOM_SEED,\n    loss_function=\"Logloss\",\n    eval_metric=\"AUC\",\n    class_weights=[1.0, spw_single],\n    od_type=\"Iter\",\n    od_wait=200,\n    task_type=\"CPU\",\n    verbose=False\n)\n\nprint(\"\\nüöÄ Training baseline CatBoost on single holdout...\")\nbaseline.fit(train_pool, eval_set=valid_pool, use_best_model=True, verbose=False)\n\nval_preds = baseline.predict_proba(X_valid)[:, 1]\nval_auc = roc_auc_score(y_valid, val_preds)\nval_logloss = log_loss(y_valid, val_preds)\n\nprint(f\"\\n‚úÖ Single-Split Validation AUC: {val_auc:.5f}  (Higher = Better)\")\nprint(f\"‚úÖ Single-Split Validation LogLoss: {val_logloss:.5f}  (Lower = Better)\")\n\n# Feature importances (top 25)\ntry:\n    imp = baseline.get_feature_importance(prettified=True)\n    imp_top = imp.sort_values(by=\"Importances\", ascending=False).head(25)\n    plt.figure(figsize=(8,6))\n    plt.barh(imp_top[\"Feature Id\"][::-1], imp_top[\"Importances\"][::-1])\n    plt.title(\"Top 25 Feature Importances ‚Äî CatBoost (Single-Split Baseline)\")\n    plt.tight_layout()\n    plt.show()\nexcept Exception as e:\n    print(\"Feature importance plot skipped:\", e)\n\n# Quick baseline submission (optional)\ntry:\n    test_preds_baseline = baseline.predict_proba(test.drop(columns=[ID_COL, DATE_COL]))[:, 1]\n    pd.DataFrame({ID_COL: test[ID_COL], \"churn\": test_preds_baseline}).to_csv(\"submission_baseline.csv\", index=False)\n    print(\"üíæ submission_baseline.csv saved.\")\nexcept Exception as e:\n    print(\"Baseline submission skipped:\", e)\n\n# ---------------------------------------------------------------\n# 7) üèÅ Auto-Retrain Final Model on ALL Data with Best Params\n# ---------------------------------------------------------------\nbest_row = grid_df.loc[grid_df[\"AUC\"].idxmax()]\nbest_depth = int(best_row[\"depth\"])\nbest_lr    = float(best_row[\"learning_rate\"])\nbest_l2    = float(best_row[\"l2_leaf_reg\"])\n\nprint(f\"\\nüèÜ Best tuned parameters from TimeCV:\")\nprint(f\"Depth = {best_depth}, Learning Rate = {best_lr}, L2 = {best_l2}\")\nprint(f\"Mean TimeCV AUC: {best_row['AUC']:.5f} | Mean LogLoss: {best_row['LogLoss']:.5f}\")\n\n# Recompute class weight on full training\nneg_full, pos_full = np.bincount(train[TARGET].astype(int))\nspw_full = max(1.0, neg_full / max(1, pos_full))\nprint(f\"\\n‚öñÔ∏è Full-data class ratio: {pos_full}/{neg_full} ‚Üí class_weight={spw_full:.2f}\")\n\nX_full = train.drop(columns=[ID_COL, DATE_COL, TARGET])\ny_full = train[TARGET]\nfull_pool = Pool(X_full, y_full, cat_features=cat_idx)\n\nfinal_model = CatBoostClassifier(\n    iterations=3000,\n    depth=best_depth,\n    learning_rate=best_lr,\n    l2_leaf_reg=best_l2,\n    random_seed=RANDOM_SEED,\n    loss_function=\"Logloss\",\n    eval_metric=\"AUC\",\n    class_weights=[1.0, spw_full],\n    od_type=\"Iter\",\n    od_wait=200,\n    task_type=\"CPU\",\n    verbose=False\n)\n\nprint(\"\\nüöÄ Training final CatBoost model on ALL training data...\")\nfinal_model.fit(full_pool)\nprint(\"‚úÖ Final model trained successfully!\")\n\n# Predict test & save final tuned submission\ntest_preds_final = final_model.predict_proba(test.drop(columns=[ID_COL, DATE_COL]))[:, 1]\nsubmission_final = pd.DataFrame({ID_COL: test[ID_COL], \"churn\": test_preds_final})\nsubmission_final.to_csv(\"submission_final.csv\", index=False)\nprint(\"\\nüíæ submission_final.csv saved successfully!\")\n\n# ---------------------------------------------------------------\n# üìä Final Summary\n# ---------------------------------------------------------------\nprint(\"\\nüìä Summary:\")\nprint(f\"- Single-Split AUC:      {val_auc:.5f}  (‚Üë higher is better)\")\nprint(f\"- Single-Split LogLoss:  {val_logloss:.5f}  (‚Üì lower is better)\")\nprint(f\"- Best TimeCV Mean AUC:  {best_row['AUC']:.5f}\")\nprint(\"- Final model retrained on ALL data with best params from TimeCV.\")\nprint(\"Files created: catboost_timecv_results.csv, submission_baseline.csv, submission_final.csv\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-19T06:58:11.121072Z","iopub.execute_input":"2025-10-19T06:58:11.121497Z","iopub.status.idle":"2025-10-19T07:00:28.967175Z","shell.execute_reply.started":"2025-10-19T06:58:11.121465Z","shell.execute_reply":"2025-10-19T07:00:28.965635Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Data loaded successfully!\nTrain shape: (133287, 87)\nTest shape:  (43006, 86)\n\nüìÖ Train size: (118422, 84), Valid size: (14865, 84)\nüß© Using 6 categorical features: ['gender', 'province', 'religion', 'work_type', 'work_sector', 'season']\n‚úÖ Built 4 time folds\n\nüîç Evaluating 27 parameter combinations over 4 folds...\n\n  Fold 1: AUC=0.71057  LogLoss=0.65502\n  Fold 2: AUC=0.70227  LogLoss=0.65398\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/951423283.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m         )\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_best_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_va\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5243\u001b[0m             \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_function'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5245\u001b[0;31m         self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[0m\u001b[1;32m   5246\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5247\u001b[0m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mplot_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training plots'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m                 self._train(\n\u001b[0m\u001b[1;32m   2411\u001b[0m                     \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m                     \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_sets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}